- 优化器的选择？  为啥都用RMSprop  Adam?
- 卷积核的数量，每层之后倍增？
- 最后一层只有一个输出？？
- 图片预处理
- 验证集Loss震荡
  - 减小LR
  - 图片转置是否有影响？
  - 网络还没收敛？
- 清理显存

  - from keras import backend as K
    K.clear_session()
- 【Keras】训练时显存out of memory的解决办法——fit_generator函数 - chenghtao的博客 - CSDN博客
  https://blog.csdn.net/chenghtao/article/details/77684958
- 炸显存

  - `OOM when allocating tensor with shape[2097152,1024]`
- `model.summary()`输出模型各层的参数状况
  - 好用，仔细看，看有没有网络建错的地方
- ==我搭的神经网络不work该怎么办！看看这11条新手最容易犯的错误 - 知乎==
  https://zhuanlan.zhihu.com/p/29247151
- 注意Conv2D的`data_format="channels_first"`所有的都要写
- How do I use the Tensorboard callback of Keras? - Stack Overflow
  https://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras
- ~~Pooling层后面要加dropout，否则会过拟合~~
  - Instead, you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.
  - Don’t Use Dropout in Convolutional Networks. – Towards Data Science
    https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16
  - dropout放在pooling之前是不对的
  - Comparing model performance: Including Max Pooling and Dropout Layers | Jessica Yung
    http://www.jessicayung.com/comparing-model-performance-including-max-pooling-and-dropout-layers/
    - pooling之后加dropout是有用的
  - 放在pooling之后，性能震荡于随机池化
    - 《Max-Pooling Dropout for Regularization of Convolutional Neural Networks》

![1552465916704](C:/Users/Simon/AppData/Roaming/Typora/typora-user-images/1552465916704.png)

- 添加batch normalization

  ```python
  model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', input_shape = (3, ROWS, COLS), data_format="channels_first"))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  ```

- Keras tensorboard
  - keras的tensorboard的可视化和模型可视化、解决No dashboards are active for the current data set等问题 - JohnieLi的博客 - CSDN博客
    https://blog.csdn.net/johinieli/article/details/80070071



​	

machine learning - Difference between Conv2D and Convolution2D in Keras - Stack Overflow
https://stackoverflow.com/questions/47414651/difference-between-conv2d-and-convolution2d-in-keras

```python
Convolution1D = Conv1D
Convolution2D = Conv2D
Convolution3D = Conv3D
SeparableConvolution2D = SeparableConv2D
Convolution2DTranspose = Conv2DTranspose
Deconvolution2D = Deconv2D = Conv2DTranspose
Deconvolution3D = Deconv3D = Conv3DTranspose

MaxPool2D = MaxPooling2D
```



keras 中的 verbose 参数详解 - 小C的博客 - CSDN博客
https://blog.csdn.net/C_chuxin/article/details/84573398



第一天-模型设计有误-100epoh

![](https://pic.superbed.cn/item/5c91ec333a213b0417c873b5)

```python
# 加dropout
Train on 21250 samples, validate on 3750 samples
Epoch 1/30
21250/21250 [==============================] - 12s 556us/step - loss: 0.8116 - acc: 0.5004 - val_loss: 0.6939 - val_acc: 0.4893
Epoch 2/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.6877 - acc: 0.5378 - val_loss: 0.6641 - val_acc: 0.6035
Epoch 3/30
21250/21250 [==============================] - 10s 492us/step - loss: 0.6591 - acc: 0.6064 - val_loss: 0.7336 - val_acc: 0.6141
Epoch 4/30
21250/21250 [==============================] - 10s 490us/step - loss: 0.6370 - acc: 0.6448 - val_loss: 0.7354 - val_acc: 0.5811
Epoch 5/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.6242 - acc: 0.6576 - val_loss: 0.7254 - val_acc: 0.6088
Epoch 6/30
21250/21250 [==============================] - 10s 489us/step - loss: 0.6125 - acc: 0.6690 - val_loss: 1.3620 - val_acc: 0.5667
Epoch 7/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.6058 - acc: 0.6783 - val_loss: 0.8382 - val_acc: 0.5760

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 8/30
21250/21250 [==============================] - 10s 490us/step - loss: 0.5851 - acc: 0.6930 - val_loss: 0.5878 - val_acc: 0.6979
Epoch 9/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5755 - acc: 0.6985 - val_loss: 0.6907 - val_acc: 0.5989
Epoch 10/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5687 - acc: 0.7069 - val_loss: 0.5942 - val_acc: 0.6928
Epoch 11/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5652 - acc: 0.7085 - val_loss: 0.5827 - val_acc: 0.7032
Epoch 12/30
21250/21250 [==============================] - 10s 492us/step - loss: 0.5599 - acc: 0.7141 - val_loss: 0.5857 - val_acc: 0.6856
Epoch 13/30
21250/21250 [==============================] - 10s 487us/step - loss: 0.5572 - acc: 0.7170 - val_loss: 0.6206 - val_acc: 0.6539
Epoch 14/30
21250/21250 [==============================] - 10s 490us/step - loss: 0.5548 - acc: 0.7167 - val_loss: 0.5759 - val_acc: 0.7027
Epoch 15/30
21250/21250 [==============================] - 10s 492us/step - loss: 0.5518 - acc: 0.7161 - val_loss: 0.5709 - val_acc: 0.7147
Epoch 16/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5468 - acc: 0.7240 - val_loss: 0.6847 - val_acc: 0.5920
Epoch 17/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5429 - acc: 0.7288 - val_loss: 0.5636 - val_acc: 0.7152
Epoch 18/30
21250/21250 [==============================] - 10s 493us/step - loss: 0.5415 - acc: 0.7289 - val_loss: 0.5545 - val_acc: 0.7216
Epoch 19/30
21250/21250 [==============================] - 10s 490us/step - loss: 0.5355 - acc: 0.7343 - val_loss: 0.5759 - val_acc: 0.7043
Epoch 20/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5351 - acc: 0.7324 - val_loss: 1.0656 - val_acc: 0.5523
Epoch 21/30
21250/21250 [==============================] - 10s 493us/step - loss: 0.5373 - acc: 0.7328 - val_loss: 0.5906 - val_acc: 0.7125
Epoch 22/30
21250/21250 [==============================] - 10s 489us/step - loss: 0.5267 - acc: 0.7397 - val_loss: 0.6849 - val_acc: 0.6515

Epoch 00022: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 23/30
21250/21250 [==============================] - 10s 490us/step - loss: 0.5177 - acc: 0.7450 - val_loss: 0.6554 - val_acc: 0.6592
Epoch 24/30
21250/21250 [==============================] - 10s 490us/step - loss: 0.5135 - acc: 0.7469 - val_loss: 0.5850 - val_acc: 0.7144
Epoch 25/30
21250/21250 [==============================] - 10s 494us/step - loss: 0.5122 - acc: 0.7509 - val_loss: 0.5599 - val_acc: 0.7261
Epoch 26/30
21250/21250 [==============================] - 10s 493us/step - loss: 0.5098 - acc: 0.7501 - val_loss: 0.6063 - val_acc: 0.6907
Epoch 27/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5069 - acc: 0.7494 - val_loss: 0.5519 - val_acc: 0.7315
Epoch 28/30
21250/21250 [==============================] - 10s 492us/step - loss: 0.5071 - acc: 0.7540 - val_loss: 0.6605 - val_acc: 0.6584
Epoch 29/30
21250/21250 [==============================] - 10s 491us/step - loss: 0.5031 - acc: 0.7591 - val_loss: 0.5872 - val_acc: 0.7181
Epoch 30/30
21250/21250 [==============================] - 10s 492us/step - loss: 0.5023 - acc: 0.7527 - val_loss: 0.6546 - val_acc: 0.6720
12500/12500 [==============================] - 4s 328us/step
```



```python
# 加了一层卷积
Train on 21250 samples, validate on 3750 samples
Epoch 1/30
21250/21250 [==============================] - 25s 1ms/step - loss: 0.7192 - acc: 0.4998 - val_loss: 0.6930 - val_acc: 0.5443
Epoch 2/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.6870 - acc: 0.5386 - val_loss: 0.6935 - val_acc: 0.5960
Epoch 3/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.6588 - acc: 0.6127 - val_loss: 0.6955 - val_acc: 0.6373
Epoch 4/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.6444 - acc: 0.6310 - val_loss: 0.6511 - val_acc: 0.6213
Epoch 5/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.6303 - acc: 0.6459 - val_loss: 0.9046 - val_acc: 0.5960
Epoch 6/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.6164 - acc: 0.6638 - val_loss: 0.6224 - val_acc: 0.6560
Epoch 7/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.6066 - acc: 0.6721 - val_loss: 0.9841 - val_acc: 0.5429
Epoch 8/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5968 - acc: 0.6764 - val_loss: 0.6059 - val_acc: 0.6755
Epoch 9/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5901 - acc: 0.6880 - val_loss: 0.7651 - val_acc: 0.5912
Epoch 10/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5796 - acc: 0.6939 - val_loss: 0.6206 - val_acc: 0.6789
Epoch 11/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5723 - acc: 0.7005 - val_loss: 0.6141 - val_acc: 0.6496
Epoch 12/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5697 - acc: 0.7058 - val_loss: 0.7253 - val_acc: 0.6187
Epoch 13/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5634 - acc: 0.7102 - val_loss: 0.8022 - val_acc: 0.6024
Epoch 14/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5603 - acc: 0.7132 - val_loss: 0.5808 - val_acc: 0.7051
Epoch 15/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5535 - acc: 0.7159 - val_loss: 0.6132 - val_acc: 0.6680
Epoch 16/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5490 - acc: 0.7205 - val_loss: 1.0589 - val_acc: 0.5747
Epoch 17/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5498 - acc: 0.7246 - val_loss: 0.9555 - val_acc: 0.6224
Epoch 18/30
21250/21250 [==============================] - 23s 1ms/step - loss: 0.5422 - acc: 0.7262 - val_loss: 0.6610 - val_acc: 0.6795
```

```python
# 第一个全连接改成512
Epoch 1/30
21250/21250 [==============================] - 29s 1ms/step - loss: 0.8012 - acc: 0.5073 - val_loss: 0.6922 - val_acc: 0.5053
Epoch 2/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.6862 - acc: 0.5440 - val_loss: 0.7231 - val_acc: 0.5504
Epoch 3/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.6604 - acc: 0.6120 - val_loss: 0.7413 - val_acc: 0.5469
Epoch 4/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.6352 - acc: 0.6440 - val_loss: 0.8826 - val_acc: 0.6501
Epoch 5/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.6173 - acc: 0.6633 - val_loss: 0.7391 - val_acc: 0.6139
Epoch 6/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.6055 - acc: 0.6724 - val_loss: 1.4910 - val_acc: 0.5133
Epoch 7/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5934 - acc: 0.6862 - val_loss: 1.0958 - val_acc: 0.5464
Epoch 8/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5850 - acc: 0.6944 - val_loss: 0.6185 - val_acc: 0.6488

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 9/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5607 - acc: 0.7114 - val_loss: 0.7124 - val_acc: 0.6312
Epoch 10/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5532 - acc: 0.7190 - val_loss: 0.8489 - val_acc: 0.5595
Epoch 11/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5477 - acc: 0.7227 - val_loss: 0.6894 - val_acc: 0.6392

Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 12/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5362 - acc: 0.7324 - val_loss: 0.6989 - val_acc: 0.6485
Epoch 13/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5286 - acc: 0.7375 - val_loss: 0.7642 - val_acc: 0.6413
Epoch 14/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5291 - acc: 0.7358 - val_loss: 0.7245 - val_acc: 0.6339

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 15/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5226 - acc: 0.7392 - val_loss: 0.6559 - val_acc: 0.6736
Epoch 16/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5200 - acc: 0.7407 - val_loss: 0.6512 - val_acc: 0.6832
Epoch 17/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5186 - acc: 0.7470 - val_loss: 0.7909 - val_acc: 0.6229
Epoch 18/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5183 - acc: 0.7424 - val_loss: 0.6658 - val_acc: 0.6731
Epoch 19/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5142 - acc: 0.7465 - val_loss: 0.7246 - val_acc: 0.6499
Epoch 20/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5138 - acc: 0.7476 - val_loss: 0.7050 - val_acc: 0.6536

Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-05.
Epoch 21/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5121 - acc: 0.7468 - val_loss: 0.7407 - val_acc: 0.6515
Epoch 22/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5137 - acc: 0.7462 - val_loss: 0.6912 - val_acc: 0.6704
Epoch 23/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5116 - acc: 0.7472 - val_loss: 0.7180 - val_acc: 0.6563
Epoch 24/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5109 - acc: 0.7526 - val_loss: 0.7050 - val_acc: 0.6664
Epoch 25/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5090 - acc: 0.7496 - val_loss: 0.7278 - val_acc: 0.6605
Epoch 26/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5053 - acc: 0.7539 - val_loss: 0.6750 - val_acc: 0.6757
Epoch 27/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5069 - acc: 0.7514 - val_loss: 0.6747 - val_acc: 0.6717
Epoch 28/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5042 - acc: 0.7528 - val_loss: 0.7328 - val_acc: 0.6584
Epoch 29/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5049 - acc: 0.7504 - val_loss: 0.6929 - val_acc: 0.6669
Epoch 30/30
21250/21250 [==============================] - 26s 1ms/step - loss: 0.5040 - acc: 0.7538 - val_loss: 0.8084 - val_acc: 0.6381
12500/12500 [==============================] - 7s 539us/step
```

![](https://pic.superbed.cn/item/5c87be753a213b0417d7f7ef)

去掉dropout之前

```python
Train on 18750 samples, validate on 6250 samples
Epoch 1/30
18750/18750 [==============================] - 15s 787us/step - loss: 0.9691 - acc: 0.5292 - val_loss: 0.6559 - val_acc: 0.5626
Epoch 2/30
18750/18750 [==============================] - 11s 612us/step - loss: 0.6126 - acc: 0.6568 - val_loss: 0.5502 - val_acc: 0.7248
Epoch 3/30
18750/18750 [==============================] - 12s 617us/step - loss: 0.5524 - acc: 0.7213 - val_loss: 0.5086 - val_acc: 0.7520
Epoch 4/30
18750/18750 [==============================] - 11s 610us/step - loss: 0.5046 - acc: 0.7534 - val_loss: 0.4566 - val_acc: 0.7826
Epoch 5/30
18750/18750 [==============================] - 11s 611us/step - loss: 0.4559 - acc: 0.7877 - val_loss: 0.4284 - val_acc: 0.8022
Epoch 6/30
18750/18750 [==============================] - 11s 602us/step - loss: 0.4163 - acc: 0.8109 - val_loss: 0.3799 - val_acc: 0.8238
Epoch 7/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.3778 - acc: 0.8323 - val_loss: 0.3780 - val_acc: 0.8382
Epoch 8/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.3487 - acc: 0.8511 - val_loss: 0.3509 - val_acc: 0.8485
Epoch 9/30
18750/18750 [==============================] - 11s 609us/step - loss: 0.3160 - acc: 0.8647 - val_loss: 0.4013 - val_acc: 0.8429
Epoch 10/30
18750/18750 [==============================] - 11s 605us/step - loss: 0.2924 - acc: 0.8771 - val_loss: 0.3107 - val_acc: 0.8648
Epoch 11/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.2779 - acc: 0.8831 - val_loss: 0.2875 - val_acc: 0.8736
Epoch 12/30
18750/18750 [==============================] - 11s 610us/step - loss: 0.2624 - acc: 0.8897 - val_loss: 0.2940 - val_acc: 0.8838
Epoch 13/30
18750/18750 [==============================] - 11s 610us/step - loss: 0.2496 - acc: 0.8946 - val_loss: 0.3212 - val_acc: 0.8814
Epoch 14/30
18750/18750 [==============================] - 11s 604us/step - loss: 0.2366 - acc: 0.9036 - val_loss: 0.3252 - val_acc: 0.8915
Epoch 15/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.2219 - acc: 0.9113 - val_loss: 0.3068 - val_acc: 0.8880
Epoch 16/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.2147 - acc: 0.9142 - val_loss: 0.2657 - val_acc: 0.8944
Epoch 17/30
18750/18750 [==============================] - 11s 610us/step - loss: 0.2067 - acc: 0.9165 - val_loss: 0.2999 - val_acc: 0.8939
Epoch 18/30
18750/18750 [==============================] - 11s 606us/step - loss: 0.2009 - acc: 0.9200 - val_loss: 0.2876 - val_acc: 0.8896
Epoch 19/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.1931 - acc: 0.9238 - val_loss: 0.2416 - val_acc: 0.9058
Epoch 20/30
18750/18750 [==============================] - 11s 607us/step - loss: 0.1914 - acc: 0.9249 - val_loss: 0.3525 - val_acc: 0.8888
Epoch 21/30
18750/18750 [==============================] - 11s 602us/step - loss: 0.1862 - acc: 0.9283 - val_loss: 0.2807 - val_acc: 0.8920
Epoch 22/30
18750/18750 [==============================] - 11s 609us/step - loss: 0.1709 - acc: 0.9343 - val_loss: 0.2537 - val_acc: 0.8997

Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 23/30
18750/18750 [==============================] - 12s 622us/step - loss: 0.1180 - acc: 0.9538 - val_loss: 0.2625 - val_acc: 0.9054
Epoch 24/30
18750/18750 [==============================] - 12s 626us/step - loss: 0.1117 - acc: 0.9564 - val_loss: 0.2450 - val_acc: 0.9102
Epoch 25/30
18750/18750 [==============================] - 12s 618us/step - loss: 0.1107 - acc: 0.9597 - val_loss: 0.2994 - val_acc: 0.9098
Epoch 26/30
18750/18750 [==============================] - 11s 613us/step - loss: 0.1034 - acc: 0.9607 - val_loss: 0.3193 - val_acc: 0.9053
Epoch 27/30
18750/18750 [==============================] - 12s 621us/step - loss: 0.1058 - acc: 0.9629 - val_loss: 0.2902 - val_acc: 0.9109
Epoch 28/30
18750/18750 [==============================] - 12s 623us/step - loss: 0.0973 - acc: 0.9667 - val_loss: 0.3999 - val_acc: 0.9146
Epoch 29/30
18750/18750 [==============================] - 12s 619us/step - loss: 0.0961 - acc: 0.9658 - val_loss: 0.2365 - val_acc: 0.9098
Epoch 30/30
18750/18750 [==============================] - 12s 623us/step - loss: 0.0926 - acc: 0.9680 - val_loss: 0.3856 - val_acc: 0.8867
12500/12500 [==============================] - 3s 241us/step
```

![1552461725762](C:/Users/Simon/AppData/Roaming/Typora/typora-user-images/1552461725762.png)

删除pooling层后的dropout

```
Train on 18750 samples, validate on 6250 samples
Epoch 1/30
18750/18750 [==============================] - 18s 942us/step - loss: 0.7019 - acc: 0.5741 - val_loss: 0.6471 - val_acc: 0.6510
Epoch 2/30
18750/18750 [==============================] - 11s 609us/step - loss: 0.5714 - acc: 0.7038 - val_loss: 0.4982 - val_acc: 0.7594
Epoch 3/30
18750/18750 [==============================] - 11s 596us/step - loss: 0.4864 - acc: 0.7707 - val_loss: 0.4490 - val_acc: 0.7738
Epoch 4/30
18750/18750 [==============================] - 11s 594us/step - loss: 0.4175 - acc: 0.8106 - val_loss: 0.3828 - val_acc: 0.8291
Epoch 5/30
18750/18750 [==============================] - 11s 599us/step - loss: 0.3556 - acc: 0.8442 - val_loss: 0.4039 - val_acc: 0.8098
Epoch 6/30
18750/18750 [==============================] - 11s 592us/step - loss: 0.2999 - acc: 0.8735 - val_loss: 0.3225 - val_acc: 0.8568
Epoch 7/30
18750/18750 [==============================] - 11s 590us/step - loss: 0.2500 - acc: 0.8958 - val_loss: 0.3114 - val_acc: 0.8630
Epoch 8/30
18750/18750 [==============================] - 11s 587us/step - loss: 0.2033 - acc: 0.9174 - val_loss: 0.3713 - val_acc: 0.8600
Epoch 9/30
18750/18750 [==============================] - 11s 589us/step - loss: 0.1592 - acc: 0.9368 - val_loss: 0.3364 - val_acc: 0.8651
Epoch 10/30
18750/18750 [==============================] - 11s 583us/step - loss: 0.1251 - acc: 0.9524 - val_loss: 0.3459 - val_acc: 0.8734
Epoch 11/30
18750/18750 [==============================] - 11s 587us/step - loss: 0.1023 - acc: 0.9616 - val_loss: 0.5773 - val_acc: 0.8728
Epoch 12/30
18750/18750 [==============================] - 11s 587us/step - loss: 0.0885 - acc: 0.9673 - val_loss: 0.3981 - val_acc: 0.8720
Epoch 13/30
18750/18750 [==============================] - 11s 583us/step - loss: 0.0778 - acc: 0.9731 - val_loss: 0.4049 - val_acc: 0.8758
Epoch 14/30
18750/18750 [==============================] - 11s 584us/step - loss: 0.0715 - acc: 0.9744 - val_loss: 0.5213 - val_acc: 0.8770
Epoch 15/30
18750/18750 [==============================] - 11s 583us/step - loss: 0.0630 - acc: 0.9782 - val_loss: 0.4032 - val_acc: 0.8536
Epoch 16/30
18750/18750 [==============================] - 11s 585us/step - loss: 0.0601 - acc: 0.9802 - val_loss: 0.5106 - val_acc: 0.8754
Epoch 17/30
18750/18750 [==============================] - 11s 582us/step - loss: 0.0549 - acc: 0.9812 - val_loss: 0.5760 - val_acc: 0.8691

Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 18/30
18750/18750 [==============================] - 11s 583us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.8101 - val_acc: 0.8906
Epoch 19/30
18750/18750 [==============================] - 11s 584us/step - loss: 0.0155 - acc: 0.9957 - val_loss: 0.5854 - val_acc: 0.8882
Epoch 20/30
18750/18750 [==============================] - 11s 582us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.6829 - val_acc: 0.8856
Epoch 21/30
18750/18750 [==============================] - 11s 582us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.9739 - val_acc: 0.8904

Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 22/30
18750/18750 [==============================] - 11s 581us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 1.0024 - val_acc: 0.8861
Epoch 23/30
18750/18750 [==============================] - 11s 589us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 1.1823 - val_acc: 0.8878
Epoch 24/30
18750/18750 [==============================] - 11s 582us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.9271 - val_acc: 0.8848

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 25/30
18750/18750 [==============================] - 11s 582us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 1.2389 - val_acc: 0.8888
Epoch 26/30
18750/18750 [==============================] - 11s 579us/step - loss: 3.4843e-05 - acc: 1.0000 - val_loss: 1.2326 - val_acc: 0.8925
Epoch 27/30
18750/18750 [==============================] - 11s 580us/step - loss: 7.5502e-07 - acc: 1.0000 - val_loss: 1.2780 - val_acc: 0.8904
Epoch 28/30
18750/18750 [==============================] - 11s 580us/step - loss: 1.7518e-06 - acc: 1.0000 - val_loss: 1.3183 - val_acc: 0.8926
Epoch 29/30
18750/18750 [==============================] - 11s 582us/step - loss: 2.1446e-07 - acc: 1.0000 - val_loss: 1.3141 - val_acc: 0.8934
Epoch 30/30
18750/18750 [==============================] - 11s 581us/step - loss: 2.1274e-04 - acc: 0.9999 - val_loss: 1.3267 - val_acc: 0.8902
12500/12500 [==============================] - 4s 344us/step
```

![1552462380650](C:/Users/Simon/AppData/Roaming/Typora/typora-user-images/1552462380650.png)

---

去掉pooling层后面的dropout，在卷积和激活函数之间添加Batch Normalization

```
Train on 18750 samples, validate on 6250 samples
Epoch 1/30
18750/18750 [==============================] - 28s 1ms/step - loss: 0.6801 - acc: 0.5857 - val_loss: 0.7410 - val_acc: 0.5323
Epoch 2/30
18750/18750 [==============================] - 17s 910us/step - loss: 0.5565 - acc: 0.7226 - val_loss: 0.7390 - val_acc: 0.5830
Epoch 3/30
18750/18750 [==============================] - 17s 909us/step - loss: 0.4647 - acc: 0.7885 - val_loss: 0.4107 - val_acc: 0.8210
Epoch 4/30
18750/18750 [==============================] - 17s 908us/step - loss: 0.4091 - acc: 0.8225 - val_loss: 0.4101 - val_acc: 0.8210
Epoch 5/30
18750/18750 [==============================] - 17s 909us/step - loss: 0.3611 - acc: 0.8461 - val_loss: 0.3590 - val_acc: 0.8416
Epoch 6/30
18750/18750 [==============================] - 17s 908us/step - loss: 0.3267 - acc: 0.8644 - val_loss: 0.5554 - val_acc: 0.7854
Epoch 7/30
18750/18750 [==============================] - 17s 908us/step - loss: 0.2963 - acc: 0.8772 - val_loss: 0.5031 - val_acc: 0.8112
Epoch 8/30
18750/18750 [==============================] - 17s 912us/step - loss: 0.2613 - acc: 0.8945 - val_loss: 0.7612 - val_acc: 0.7382

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 9/30
18750/18750 [==============================] - 18s 942us/step - loss: 0.1794 - acc: 0.9301 - val_loss: 0.3663 - val_acc: 0.8653
Epoch 10/30
18750/18750 [==============================] - 18s 939us/step - loss: 0.1461 - acc: 0.9461 - val_loss: 0.4444 - val_acc: 0.8512
Epoch 11/30
18750/18750 [==============================] - 18s 976us/step - loss: 0.1205 - acc: 0.9555 - val_loss: 0.5672 - val_acc: 0.8264
Epoch 12/30
18750/18750 [==============================] - 18s 954us/step - loss: 0.1007 - acc: 0.9627 - val_loss: 1.1222 - val_acc: 0.8141

Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 13/30
18750/18750 [==============================] - 18s 946us/step - loss: 0.0532 - acc: 0.9826 - val_loss: 0.6469 - val_acc: 0.8192
Epoch 14/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0436 - acc: 0.9855 - val_loss: 0.7026 - val_acc: 0.8448
Epoch 15/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0328 - acc: 0.9884 - val_loss: 0.7719 - val_acc: 0.8648

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 16/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0157 - acc: 0.9946 - val_loss: 0.7206 - val_acc: 0.8558
Epoch 17/30
18750/18750 [==============================] - 18s 942us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.6715 - val_acc: 0.8920
Epoch 18/30
18750/18750 [==============================] - 18s 940us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.6500 - val_acc: 0.8882
Epoch 19/30
18750/18750 [==============================] - 18s 943us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.8772 - val_acc: 0.8501
Epoch 20/30
18750/18750 [==============================] - 18s 944us/step - loss: 0.0086 - acc: 0.9975 - val_loss: 0.8028 - val_acc: 0.8816

Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-05.
Epoch 21/30
18750/18750 [==============================] - 18s 944us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.7276 - val_acc: 0.8875
Epoch 22/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.7537 - val_acc: 0.8909
Epoch 23/30
18750/18750 [==============================] - 18s 941us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.8115 - val_acc: 0.8870
Epoch 24/30
18750/18750 [==============================] - 18s 942us/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.8080 - val_acc: 0.8811
Epoch 25/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.9244 - val_acc: 0.8675
Epoch 26/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.9608 - val_acc: 0.8880
Epoch 27/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0040 - acc: 0.9989 - val_loss: 1.0738 - val_acc: 0.8502
Epoch 28/30
18750/18750 [==============================] - 18s 945us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.9609 - val_acc: 0.8901
Epoch 29/30
18750/18750 [==============================] - 18s 944us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.9409 - val_acc: 0.8818
Epoch 30/30
18750/18750 [==============================] - 18s 943us/step - loss: 0.0031 - acc: 0.9987 - val_loss: 1.2254 - val_acc: 0.8518
12500/12500 [==============================] - 6s 482us/step
```

![1552468204463](C:/Users/Simon/AppData/Roaming/Typora/typora-user-images/1552468204463.png)

---

又去掉三层卷积（一共去掉了最后6层512channel的卷积）

网络结构

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 64, 64, 64)        1792      
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
activation_1 (Activation)    (None, 64, 64, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 64, 64)        36928     
_________________________________________________________________
batch_normalization_2 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
activation_2 (Activation)    (None, 64, 64, 64)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 32, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 128, 32, 32)       73856     
_________________________________________________________________
batch_normalization_3 (Batch (None, 128, 32, 32)       128       
_________________________________________________________________
activation_3 (Activation)    (None, 128, 32, 32)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 128, 32, 32)       147584    
_________________________________________________________________
batch_normalization_4 (Batch (None, 128, 32, 32)       128       
_________________________________________________________________
activation_4 (Activation)    (None, 128, 32, 32)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 128, 16, 16)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 256, 16, 16)       295168    
_________________________________________________________________
batch_normalization_5 (Batch (None, 256, 16, 16)       64        
_________________________________________________________________
activation_5 (Activation)    (None, 256, 16, 16)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 256, 16, 16)       590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 256, 16, 16)       64        
_________________________________________________________________
activation_6 (Activation)    (None, 256, 16, 16)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 256, 16, 16)       590080    
_________________________________________________________________
batch_normalization_7 (Batch (None, 256, 16, 16)       64        
_________________________________________________________________
activation_7 (Activation)    (None, 256, 16, 16)       0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 256, 8, 8)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 16384)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               2097280   
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
_________________________________________________________________
activation_8 (Activation)    (None, 1)                 0         
=================================================================
Total params: 3,833,857
Trainable params: 3,833,377
Non-trainable params: 480
_________________________________________________________________
```



```python
Train on 18750 samples, validate on 6250 samples
Epoch 1/30
18750/18750 [==============================] - 20s 1ms/step - loss: 0.7130 - acc: 0.5690 - val_loss: 0.6243 - val_acc: 0.6485
Epoch 2/30
18750/18750 [==============================] - 14s 760us/step - loss: 0.6027 - acc: 0.6731 - val_loss: 0.5674 - val_acc: 0.7138
Epoch 3/30
18750/18750 [==============================] - 14s 760us/step - loss: 0.5409 - acc: 0.7276 - val_loss: 0.5392 - val_acc: 0.7274
Epoch 4/30
18750/18750 [==============================] - 14s 760us/step - loss: 0.4962 - acc: 0.7635 - val_loss: 0.4722 - val_acc: 0.7643
Epoch 5/30
18750/18750 [==============================] - 14s 760us/step - loss: 0.4593 - acc: 0.7899 - val_loss: 0.4311 - val_acc: 0.8061
Epoch 6/30
18750/18750 [==============================] - 14s 752us/step - loss: 0.4261 - acc: 0.8083 - val_loss: 0.4059 - val_acc: 0.8200
Epoch 7/30
18750/18750 [==============================] - 14s 758us/step - loss: 0.3972 - acc: 0.8265 - val_loss: 0.4214 - val_acc: 0.8082
Epoch 8/30
18750/18750 [==============================] - 14s 752us/step - loss: 0.3698 - acc: 0.8375 - val_loss: 0.5735 - val_acc: 0.7322
Epoch 9/30
18750/18750 [==============================] - 14s 756us/step - loss: 0.3388 - acc: 0.8535 - val_loss: 0.4660 - val_acc: 0.7816

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 10/30
18750/18750 [==============================] - 14s 728us/step - loss: 0.2725 - acc: 0.8840 - val_loss: 0.3183 - val_acc: 0.8611
Epoch 11/30
18750/18750 [==============================] - 13s 717us/step - loss: 0.2500 - acc: 0.8954 - val_loss: 0.3171 - val_acc: 0.8720
Epoch 12/30
18750/18750 [==============================] - 13s 718us/step - loss: 0.2331 - acc: 0.9063 - val_loss: 0.4685 - val_acc: 0.8502
Epoch 13/30
18750/18750 [==============================] - 13s 712us/step - loss: 0.2168 - acc: 0.9127 - val_loss: 0.3181 - val_acc: 0.8686
Epoch 14/30
18750/18750 [==============================] - 13s 710us/step - loss: 0.2033 - acc: 0.9200 - val_loss: 0.3376 - val_acc: 0.8456

Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 15/30
18750/18750 [==============================] - 13s 705us/step - loss: 0.1584 - acc: 0.9394 - val_loss: 0.2866 - val_acc: 0.8947
Epoch 16/30
18750/18750 [==============================] - 13s 710us/step - loss: 0.1447 - acc: 0.9453 - val_loss: 0.2715 - val_acc: 0.8978
Epoch 17/30
18750/18750 [==============================] - 13s 711us/step - loss: 0.1335 - acc: 0.9482 - val_loss: 0.2811 - val_acc: 0.8966
Epoch 18/30
18750/18750 [==============================] - 13s 711us/step - loss: 0.1212 - acc: 0.9547 - val_loss: 0.3049 - val_acc: 0.8955
Epoch 19/30
18750/18750 [==============================] - 13s 715us/step - loss: 0.1128 - acc: 0.9565 - val_loss: 0.2969 - val_acc: 0.9005
Epoch 20/30
18750/18750 [==============================] - 13s 712us/step - loss: 0.1024 - acc: 0.9633 - val_loss: 0.4565 - val_acc: 0.8810
Epoch 21/30
18750/18750 [==============================] - 13s 708us/step - loss: 0.0973 - acc: 0.9644 - val_loss: 0.3264 - val_acc: 0.8987
Epoch 22/30
18750/18750 [==============================] - 13s 710us/step - loss: 0.0860 - acc: 0.9684 - val_loss: 0.3776 - val_acc: 0.8899

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 23/30
18750/18750 [==============================] - 14s 720us/step - loss: 0.0628 - acc: 0.9786 - val_loss: 0.3440 - val_acc: 0.9011
Epoch 24/30
18750/18750 [==============================] - 13s 713us/step - loss: 0.0578 - acc: 0.9806 - val_loss: 0.3401 - val_acc: 0.9026
Epoch 25/30
18750/18750 [==============================] - 13s 708us/step - loss: 0.0540 - acc: 0.9820 - val_loss: 0.3576 - val_acc: 0.9030
Epoch 26/30
18750/18750 [==============================] - 13s 710us/step - loss: 0.0482 - acc: 0.9841 - val_loss: 0.3447 - val_acc: 0.8978
Epoch 27/30
18750/18750 [==============================] - 13s 710us/step - loss: 0.0458 - acc: 0.9850 - val_loss: 0.3723 - val_acc: 0.9022
Epoch 28/30
18750/18750 [==============================] - 13s 707us/step - loss: 0.0400 - acc: 0.9875 - val_loss: 0.4051 - val_acc: 0.8896

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-05.
Epoch 29/30
18750/18750 [==============================] - 13s 712us/step - loss: 0.0351 - acc: 0.9893 - val_loss: 0.3715 - val_acc: 0.9000
Epoch 30/30
18750/18750 [==============================] - 13s 713us/step - loss: 0.0325 - acc: 0.9902 - val_loss: 0.4090 - val_acc: 0.9067
12500/12500 [==============================] - 5s 413us/step
```

![1552470201010](C:/Users/Simon/AppData/Roaming/Typora/typora-user-images/1552470201010.png)

---

数据正则化后

```
Epoch 1/30
292/292 [==============================] - 17s 59ms/step - loss: 0.7468 - acc: 0.5478 - val_loss: 0.6412 - val_acc: 0.6482
Epoch 2/30
292/292 [==============================] - 14s 49ms/step - loss: 0.6255 - acc: 0.6611 - val_loss: 0.5909 - val_acc: 0.6827
Epoch 3/30
292/292 [==============================] - 14s 49ms/step - loss: 0.5748 - acc: 0.6992 - val_loss: 0.6284 - val_acc: 0.6986
Epoch 4/30
292/292 [==============================] - 14s 48ms/step - loss: 0.5428 - acc: 0.7277 - val_loss: 0.5071 - val_acc: 0.7509
Epoch 5/30
292/292 [==============================] - 14s 49ms/step - loss: 0.5126 - acc: 0.7524 - val_loss: 0.5344 - val_acc: 0.7494
Epoch 6/30
292/292 [==============================] - 14s 48ms/step - loss: 0.4869 - acc: 0.7668 - val_loss: 0.5077 - val_acc: 0.7694
Epoch 7/30
292/292 [==============================] - 14s 49ms/step - loss: 0.4651 - acc: 0.7838 - val_loss: 0.4298 - val_acc: 0.8042
Epoch 8/30
292/292 [==============================] - 14s 48ms/step - loss: 0.4405 - acc: 0.7979 - val_loss: 0.4546 - val_acc: 0.7958
Epoch 9/30
292/292 [==============================] - 14s 49ms/step - loss: 0.4195 - acc: 0.8110 - val_loss: 0.4431 - val_acc: 0.8064
Epoch 10/30
292/292 [==============================] - 14s 49ms/step - loss: 0.4006 - acc: 0.8229 - val_loss: 0.4320 - val_acc: 0.8162
Epoch 11/30
292/292 [==============================] - 14s 49ms/step - loss: 0.3847 - acc: 0.8331 - val_loss: 0.3914 - val_acc: 0.8336
Epoch 12/30
292/292 [==============================] - 14s 49ms/step - loss: 0.3692 - acc: 0.8371 - val_loss: 0.3597 - val_acc: 0.8410
Epoch 13/30
292/292 [==============================] - 14s 49ms/step - loss: 0.3480 - acc: 0.8489 - val_loss: 0.3422 - val_acc: 0.8530
Epoch 14/30
292/292 [==============================] - 14s 49ms/step - loss: 0.3381 - acc: 0.8538 - val_loss: 0.4601 - val_acc: 0.8285
Epoch 15/30
292/292 [==============================] - 14s 49ms/step - loss: 0.3289 - acc: 0.8584 - val_loss: 0.4210 - val_acc: 0.8494
Epoch 16/30
292/292 [==============================] - 14s 49ms/step - loss: 0.3127 - acc: 0.8678 - val_loss: 0.3506 - val_acc: 0.8685
Epoch 17/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2971 - acc: 0.8738 - val_loss: 0.3264 - val_acc: 0.8506
Epoch 18/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2896 - acc: 0.8813 - val_loss: 0.3889 - val_acc: 0.8592
Epoch 19/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2775 - acc: 0.8834 - val_loss: 0.4002 - val_acc: 0.8656

Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 20/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2440 - acc: 0.9000 - val_loss: 0.2790 - val_acc: 0.8882
Epoch 21/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2239 - acc: 0.9078 - val_loss: 0.2620 - val_acc: 0.8843
Epoch 22/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2229 - acc: 0.9083 - val_loss: 0.3275 - val_acc: 0.8818
Epoch 23/30
292/292 [==============================] - 14s 49ms/step - loss: 0.2179 - acc: 0.9090 - val_loss: 0.3538 - val_acc: 0.8800

Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 24/30
292/292 [==============================] - 14s 49ms/step - loss: 0.1957 - acc: 0.9222 - val_loss: 0.3773 - val_acc: 0.8784
Epoch 25/30
292/292 [==============================] - 14s 49ms/step - loss: 0.1935 - acc: 0.9219 - val_loss: 0.2557 - val_acc: 0.8958
Epoch 26/30
292/292 [==============================] - 14s 49ms/step - loss: 0.1911 - acc: 0.9248 - val_loss: 0.2954 - val_acc: 0.8848
Epoch 27/30
292/292 [==============================] - 14s 48ms/step - loss: 0.1869 - acc: 0.9244 - val_loss: 0.2809 - val_acc: 0.8974
Epoch 28/30
292/292 [==============================] - 14s 48ms/step - loss: 0.1836 - acc: 0.9276 - val_loss: 0.2689 - val_acc: 0.9010
Epoch 29/30
292/292 [==============================] - 14s 49ms/step - loss: 0.1789 - acc: 0.9294 - val_loss: 0.2891 - val_acc: 0.9022
Epoch 30/30
292/292 [==============================] - 14s 48ms/step - loss: 0.1798 - acc: 0.9284 - val_loss: 0.2703 - val_acc: 0.9024
12500/12500 [==============================] - 4s 352us/step
```

![1552534972634](C:/Users/Simon/AppData/Roaming/Typora/typora-user-images/1552534972634.png)